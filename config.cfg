[General]
# 日志等级: TRACE / DEBUG / WARNING / INFO / ERROR / CRITICAL
LogLevel = INFO
# 服务端口
Server_Port = 7860
# ASR 内容更新的频率 (秒)
ASR_Update_Freq = 0.05
# ASR 队列检查间隔, 如果队列为空则等待指定时间后再次检查 (秒)
ASR_QCheck_Freq = 0.05
# ASR 历史信息保留的数量
ASR_Result_History = 5
# ASR 识别时单个句子最大的 tokens 数量, 超过时会强制断句
ASR_Max_tokens_by_Sentence = 48
# FFmpeg 所在目录, 目录中需要包含 ffmpeg.exe, 通常在 bin 目录里
FFmpeg_Path = ./ffmpeg/bin

[ASR]
# 每个音频帧的长度, 必须是 512 的倍数
Frames_Per_Buffer = 1536
# ASR 识别时最长可以缓存多少个音频帧, 16000 为一秒, 总时长为 "Frames_Per_Buffer * ASR_Frames_Buffer / 16000.0"
ASR_Frames_Buffer = 48
# ASR 首次识别时的延迟, 为了提高首次识别的准确率, 该值最好不要设置为1, 延迟长度为 "ASR_Delay_Buffer * Frames_Per_Buffer / 16000.0"
ASR_Delay_Buffer = 6
# ASR 模型每次识别时最大 token 长度, 可以控制一部分延迟以及异常情况
ASR_Max_Tokens = 60
# ASR 模型路径
ASR_Model = ./model/Whisper-large-v3-turbo
# 活动语音检测的触发阈值, 范围 0 ~ 1 之间, 值越高则越不容易触发语音检测, 建议 0.6 以上
VAD_Threshold = 0.9

[LLM]
SYS_Prompt = 你是专业的语音翻译助手, 请将识别的语音内容翻译为<LANG>, 并尝试纠错误 & 保留原始时间信息
# 可以是 ollama 或 openai
Provider = ollama
# 模型名字, 推荐使用经过指令微调 (instruct) 的模型, 而不是思考 (Thinking) 模型
Model_Name = qwen3:4b-instruct
# 模型调用的 API 链接; 如果是 openai 类型的需要填写到 https://***/v1
Base_URL = http://localhost:11434
# 模型单次最大输出 token 数量
MAX_Tokens = 256
# API Key, 如果是 openai 类型的则必须设置 如果是 LM Studio 这样的本地服务器端则可以随便填写, 有一个就行)
API_Key = API_Key